{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "0fKxneQADuPQ"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import sklearn\n",
        "from sklearn.cluster import KMeans\n",
        "import scipy.cluster.vq as vq\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import struct\n",
        "import pickle\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn import neighbors, metrics\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "RrCVmqNQDuPZ",
        "outputId": "aebacd2a-80d7-4983-96f7-5145a09017e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Image-recognition' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/CyrusChiu/Image-recognition\n",
        "def load_cifar10_data(dataset):\n",
        "    if dataset == 'train':\n",
        "        with open('/content/cifar10/train/train.txt','r') as f:\n",
        "            paths = f.readlines()\n",
        "    if dataset == 'test':\n",
        "        with open('/content/cifar10/test/test.txt','r') as f:\n",
        "            paths = f.readlines()\n",
        "    x, y = [], []\n",
        "    for each in paths:\n",
        "        each = each.strip()\n",
        "        path, label = each.split(' ')\n",
        "        img = cv2.imread(path)\n",
        "        x.append(img)\n",
        "        y.append(label)\n",
        "    return [x, y]\n",
        "\n",
        "def load_my_data(path, test=None):\n",
        "    with open(path, 'r') as f:\n",
        "        paths = f.readlines()\n",
        "    x, y = [], []\n",
        "    for each in paths:\n",
        "        each = each.strip()\n",
        "        label, path = each.split(' ')\n",
        "        img = cv2.imread(path)\n",
        "        if img.shape[:2] != (256,256):\n",
        "            img = cv2.resize(img, (256,256))\n",
        "        x.append(img)\n",
        "        y.append(label)\n",
        "    return [x, y]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Lw1E3OgNDuPb"
      },
      "outputs": [],
      "source": [
        "def extract_DenseSift_descriptors(img):\n",
        "\n",
        "    gray = img\n",
        "\n",
        "    sift = cv2.SIFT_create()\n",
        "    disft_step_size = DSIFT_STEP_SIZE\n",
        "    keypoints = [cv2.KeyPoint(x, y, disft_step_size)\n",
        "            for y in range(0, gray.shape[0], disft_step_size)\n",
        "                for x in range(0, gray.shape[1], disft_step_size)]\n",
        "\n",
        "    keypoints, descriptors = sift.compute(gray, keypoints)\n",
        "\n",
        "    return [keypoints, descriptors]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "D8XldzrHDuPc"
      },
      "outputs": [],
      "source": [
        "def svm_classifier(x_train, y_train, x_test=None, y_test=None):\n",
        "\n",
        "    C_range = 10.0 ** np.arange(-3, 3)\n",
        "    gamma_range = 10.0 ** np.arange(-3, 3)\n",
        "    param_grid = dict(gamma=gamma_range.tolist(), C=C_range.tolist())\n",
        "\n",
        "    # Grid search for C, gamma, 5-fold CV\n",
        "    print(\"Tuning hyper-parameters\\n\")\n",
        "    clf = GridSearchCV(svm.SVC(), param_grid, cv=5, n_jobs=-2)\n",
        "    clf.fit(x_train, y_train)\n",
        "    print(\"Best parameters set found on development set:\\n\")\n",
        "    print(clf.best_estimator_)\n",
        "    print(\"\\nGrid scores on development set:\\n\")\n",
        "    means = clf.cv_results_['mean_test_score']\n",
        "    stds = clf.cv_results_['std_test_score']\n",
        "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
        "              % (mean, std * 2, params))\n",
        "    print(\"\\nDetailed classification report:\\n\")\n",
        "    print(\"The model is trained on the full development set.\")\n",
        "    print(\"The scores are computed on the full evaluation set.\\n\")\n",
        "    y_true, y_pred = y_test, clf.predict(x_test)\n",
        "    #print(classification_report(y_true, y_pred, target_names=get_label()))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    return y_true, y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "llJFTL21DuPe"
      },
      "outputs": [],
      "source": [
        "def build_codebook(X, voc_size):\n",
        "\n",
        "    features = np.vstack([descriptor for descriptor in X])\n",
        "    kmeans = KMeans(n_clusters=voc_size)\n",
        "    kmeans.fit(features)\n",
        "    codebook = kmeans.cluster_centers_.squeeze()\n",
        "    print(\"Codebook Building Complete\")\n",
        "    return codebook\n",
        "\n",
        "def input_vector_encoder(feature, codebook):\n",
        "    \"\"\"\n",
        "    Input all the local feature of the image\n",
        "    Pooling (encoding) by codebook and return\n",
        "    \"\"\"\n",
        "    code, _ = vq.vq(feature, codebook)\n",
        "    word_hist, bin_edges = np.histogram(code, bins=range(codebook.shape[0] + 1), density=True)\n",
        "    return word_hist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "GabId2acDuPg"
      },
      "outputs": [],
      "source": [
        "def build_spatial_pyramid(image, descriptor, level):\n",
        "    \"\"\"\n",
        "    Rebuild the descriptors according to the level of pyramid\n",
        "    \"\"\"\n",
        "    assert 0 <= level <= 2, \"Level Error\"\n",
        "    step_size = DSIFT_STEP_SIZE\n",
        "    #from utils import DSIFT_STEP_SIZE as s\n",
        "    s = 4\n",
        "    assert s == step_size, \"step_size must equal to DSIFT_STEP_SIZE in utils.extract_DenseSift_descriptors()\"\n",
        "    h = image.shape[0] // step_size\n",
        "    w = image.shape[1] // step_size\n",
        "    idx_crop = np.array(range(len(descriptor))).reshape(h,w)\n",
        "    size = idx_crop.itemsize\n",
        "    height, width = idx_crop.shape\n",
        "    bh, bw = 2**(3-level), 2**(3-level)\n",
        "    shape = (height//bh, width//bw, bh, bw)\n",
        "    strides = size * np.array([width*bh, bw, width, 1])\n",
        "    crops = np.lib.stride_tricks.as_strided(\n",
        "            idx_crop, shape=shape, strides=strides)\n",
        "    des_idxs = [col_block.flatten().tolist() for row_block in crops\n",
        "                for col_block in row_block]\n",
        "    pyramid = []\n",
        "    for idxs in des_idxs:\n",
        "        pyramid.append(np.asarray([descriptor[idx] for idx in idxs]))\n",
        "    return pyramid\n",
        "\n",
        "\n",
        "# In[3]:\n",
        "\n",
        "\n",
        "def spatial_pyramid_matching(image, descriptor, codebook, level):\n",
        "    pyramid = []\n",
        "    if level == 0:\n",
        "        pyramid += build_spatial_pyramid(image, descriptor, level=0)\n",
        "        code = [input_vector_encoder(crop, codebook) for crop in pyramid]\n",
        "        return np.asarray(code).flatten()\n",
        "    if level == 1:\n",
        "        pyramid += build_spatial_pyramid(image, descriptor, level=0)\n",
        "        pyramid += build_spatial_pyramid(image, descriptor, level=1)\n",
        "        code = [input_vector_encoder(crop, codebook) for crop in pyramid]\n",
        "        code_level_0 = 0.5 * np.asarray(code[0]).flatten()\n",
        "        code_level_1 = 0.5 * np.asarray(code[1:]).flatten()\n",
        "        return np.concatenate((code_level_0, code_level_1))\n",
        "    if level == 2:\n",
        "        pyramid += build_spatial_pyramid(image, descriptor, level=0)\n",
        "        pyramid += build_spatial_pyramid(image, descriptor, level=1)\n",
        "        pyramid += build_spatial_pyramid(image, descriptor, level=2)\n",
        "        code = [input_vector_encoder(crop, codebook) for crop in pyramid]\n",
        "        code_level_0 = 0.25 * np.asarray(code[0]).flatten()\n",
        "        code_level_1 = 0.25 * np.asarray(code[1:5]).flatten()\n",
        "        code_level_2 = 0.5 * np.asarray(code[5:]).flatten()\n",
        "        return np.concatenate((code_level_0, code_level_1, code_level_2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "yDbv9BesDuPh"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    accuracy = np.mean(np.max(cm,axis=1))\n",
        "    print(accuracy)\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Poscx8ARDuPj"
      },
      "outputs": [],
      "source": [
        "#root = './data'\n",
        "#download = True\n",
        "\n",
        "#trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
        "#train_set = dset.MNIST(root=root, train=True, transform=trans, download=download)\n",
        "#test_set = dset.MNIST(root=root, train=False, transform=trans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "CL7QqNGwDuPk",
        "outputId": "902af5ed-99c6-4b0d-ec04-3b136a8ce26b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-6a24037f0b58>:5: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
            "  return np.fromstring(f.read(), dtype=np.uint8).reshape(shape)\n"
          ]
        }
      ],
      "source": [
        "#raw_train = read_idx(\"/content/data/MNIST/raw/train-images-idx3-ubyte\")\n",
        "#train_data = np.reshape(raw_train,(60000,28*28))\n",
        "#train_label = read_idx(\"/content/data/MNIST/raw/train-labels-idx1-ubyte\")\n",
        "\n",
        "#raw_test = read_idx(\"/content/data/MNIST/raw/t10k-images-idx3-ubyte\")\n",
        "#test_data = np.reshape(raw_test,(10000,28*28))\n",
        "#test_label = read_idx(\"/content/data/MNIST/raw/t10k-labels-idx1-ubyte\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiIAQtD0DuPm"
      },
      "outputs": [],
      "source": [
        "#x_train = []\n",
        "#y_train = train_label\n",
        "#x_test = []\n",
        "#y_test = test_label\n",
        "\n",
        "#for i in range(len(train_data)):\n",
        "#    img1 = train_data[i].reshape((28,28))\n",
        "#    img1 = np.uint8(img1*255)\n",
        "#    x_train.append(img1)\n",
        "\n",
        "#for j in range(len(test_data)):\n",
        "#    img2 = test_data[j].reshape((28,28))\n",
        "#    img2 = np.uint8(img2*255)\n",
        "#    x_test.append(img2)\n",
        "#\n",
        "#x_train = np.asarray(x_train)\n",
        "#x_test = np.asarray(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kg30DZC2DuPn",
        "outputId": "9db0164d-fc0e-40be-c38f-e0f41bb6ca1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 28, 28)\n",
            "(2000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "#x_train = x_train[:10000,:,:]\n",
        "#x_test = x_test[:2000,:,:]\n",
        "#y_train = y_train[:10000]\n",
        "#y_test = y_test[:2000]\n",
        "#print (x_train.shape)\n",
        "#print (x_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5X1qlPPDuPn",
        "outputId": "ffe4f622-b56a-48a5-e6bb-a27533139528",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Test split: 50000/10000\n",
            "Codebook Size: 100\n",
            "Pyramid level: 1\n",
            "Building the codebook, it will take some time\n"
          ]
        }
      ],
      "source": [
        "VOC_SIZE = 100\n",
        "PYRAMID_LEVEL = 1\n",
        "DSIFT_STEP_SIZE = 4\n",
        "\n",
        "x_train, y_train = load_cifar10_data(dataset='train')\n",
        "x_test, y_test = load_cifar10_data(dataset='test')\n",
        "\n",
        "x_train_feature = [extract_DenseSift_descriptors(img) for img in x_train]\n",
        "x_test_feature = [extract_DenseSift_descriptors(img) for img in x_test]\n",
        "x_train_kp, x_train_des = zip(*x_train_feature)\n",
        "x_test_kp, x_test_des = zip(*x_test_feature)\n",
        "print (\"Train/Test split: {:d}/{:d}\".format(len(y_train), len(y_test)))\n",
        "print (\"Codebook Size: {:d}\".format(VOC_SIZE))\n",
        "print (\"Pyramid level: {:d}\".format(PYRAMID_LEVEL))\n",
        "print (\"Building the codebook, it will take some time\")\n",
        "codebook = build_codebook(x_train_des, VOC_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTqoBc1QDuPo"
      },
      "outputs": [],
      "source": [
        "with open('./spm_lv1_codebook.pkl','wb') as f:\n",
        "    pickle.dump(codebook, f)\n",
        "\n",
        "print (\"Spatial Pyramid Matching encoding\")\n",
        "print (x_train.shape)\n",
        "print (x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4WZiWRuDuPp"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# with open('./spm_lv1_codebook.pkl','rb') as f:\n",
        "#     codebook=pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbHhvH7ZDuPp"
      },
      "outputs": [],
      "source": [
        "x_train = [spatial_pyramid_matching(x_train[i],\n",
        "                                    x_train_des[i],\n",
        "                                    codebook,\n",
        "                                    level=PYRAMID_LEVEL)\n",
        "                                    for i in range(len(x_train))]\n",
        "\n",
        "x_test = [spatial_pyramid_matching(x_test[i],x_test_des[i],codebook,level=PYRAMID_LEVEL) for i in range(len(x_test))]\n",
        "\n",
        "x_train = np.asarray(x_train)\n",
        "x_test = np.asarray(x_test)\n",
        "\n",
        "y_true,y_pred = svm_classifier(x_train, y_train, x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO77c1EhDuPq"
      },
      "outputs": [],
      "source": [
        "cm = metrics.confusion_matrix(y_true, y_pred)\n",
        "accuracy=plot_confusion_matrix(cm,[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"],normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zak_jJsNDuPq"
      },
      "outputs": [],
      "source": [
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZH_0StlDuPq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}